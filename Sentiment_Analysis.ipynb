{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUyS0ZehYwN7"
      },
      "source": [
        "# **Sentiment Analysis On Movie Reviews**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeORIBMdYo21"
      },
      "source": [
        "**1. Dataset Collection**\n",
        "\n",
        "Zipfile Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1rhZSZ46XlOi",
        "outputId": "5082605e-9804-4203-86e9-e2c594c18a26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[]\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Define the path to the zip file\n",
        "zip_file_path = 'C:/Users/BHANUTEJA/Desktop/Sentiment Analysis/Movie_reviews.zip'  # Replace with your actual path\n",
        "\n",
        "# Extract the zip file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"C:/Users/BHANUTEJA/Desktop/Sentiment Analysis/\")  # Extract files to the content folder\n",
        "\n",
        "# List the extracted files to confirm\n",
        "extracted_files = os.listdir(\"C:/Users/BHANUTEJA/Desktop/Sentiment Analysis/content/\")\n",
        "print(extracted_files)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuwofRHJfT4A"
      },
      "source": [
        "**2. Data Preprocessing**\n",
        "\n",
        "We'll load the data, inspect it, and clean it up if necessary. Typically, the steps involved in preprocessing are:\n",
        "\n",
        "**Loading the data:** We'll load the dataset into a pandas DataFrame.\n",
        "\n",
        "**Inspecting the data:** Check for missing values, irrelevant columns, and basic statistics.\n",
        "\n",
        "**Text cleaning:** Remove unwanted characters, stopwords, and normalize the text.\n",
        "\n",
        "**Label encoding:** Convert the target variable (positive/negative) into numerical format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "M-YzwQWdZfwJ",
        "outputId": "ec25e7d9-ffc8-4c80-cc53-1454c5f29864"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in c:\\udemy\\python\\venv\\lib\\site-packages (2.2.3)\n",
            "Requirement already satisfied: numpy>=1.26.0 in c:\\udemy\\python\\venv\\lib\\site-packages (from pandas) (2.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\udemy\\python\\venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\udemy\\python\\venv\\lib\\site-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\udemy\\python\\venv\\lib\\site-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in c:\\udemy\\python\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review sentiment\n",
              "0  One of the other reviewers has mentioned that ...  positive\n",
              "1  A wonderful little production. <br /><br />The...  positive\n",
              "2  I thought this was a wonderful way to spend ti...  positive\n",
              "3  Basically there's a family where a little boy ...  negative\n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "!pip install pandas\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "file_path = 'C:/Users/BHANUTEJA/Desktop/Sentiment Analysis/IMDB Dataset.csv'  # Update with your actual file path\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Display the first few rows of the dataset to understand its structure\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l07I1sfdfGFA"
      },
      "source": [
        "**3. Text Cleaning**\n",
        "\n",
        "In sentiment analysis, it's essential to clean the text data to remove noise and irrelevant information. The typical steps are:\n",
        "\n",
        "Convert text to lowercase\n",
        "\n",
        "Remove punctuation, special characters, and numbers\n",
        "\n",
        "Tokenize text (split into words)\n",
        "\n",
        "Remove stopwords (commonly used words like 'the', 'is', 'in', etc.)\n",
        "\n",
        "Lemmatization (reduce words to their base form, e.g., 'running' to 'run')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "id": "oyUPATnscb9M",
        "outputId": "344f9da4-67e7-4fbd-a152-0a7a87e6c70d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting nltk\n",
            "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting click (from nltk)\n",
            "  Using cached click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting joblib (from nltk)\n",
            "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting regex>=2021.8.3 (from nltk)\n",
            "  Downloading regex-2024.11.6-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
            "     ---------------------------------------- 0.0/41.5 kB ? eta -:--:--\n",
            "     --------- ------------------------------ 10.2/41.5 kB ? eta -:--:--\n",
            "     ---------------------------- --------- 30.7/41.5 kB 325.1 kB/s eta 0:00:01\n",
            "     -------------------------------------- 41.5/41.5 kB 284.6 kB/s eta 0:00:00\n",
            "Collecting tqdm (from nltk)\n",
            "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
            "     ---------------------------------------- 0.0/57.7 kB ? eta -:--:--\n",
            "     ---------------------------------------- 57.7/57.7 kB 1.5 MB/s eta 0:00:00\n",
            "Requirement already satisfied: colorama in c:\\udemy\\python\\venv\\lib\\site-packages (from click->nltk) (0.4.6)\n",
            "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
            "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
            "   ----- ---------------------------------- 0.2/1.5 MB 5.9 MB/s eta 0:00:01\n",
            "   ---------- ----------------------------- 0.4/1.5 MB 5.1 MB/s eta 0:00:01\n",
            "   ----------------- ---------------------- 0.7/1.5 MB 6.0 MB/s eta 0:00:01\n",
            "   ------------------------ --------------- 0.9/1.5 MB 5.8 MB/s eta 0:00:01\n",
            "   ---------------------------------- ----- 1.3/1.5 MB 5.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 1.5/1.5 MB 6.0 MB/s eta 0:00:00\n",
            "Downloading regex-2024.11.6-cp312-cp312-win_amd64.whl (273 kB)\n",
            "   ---------------------------------------- 0.0/273.6 kB ? eta -:--:--\n",
            "   ---------------------------------------- 273.6/273.6 kB 8.5 MB/s eta 0:00:00\n",
            "Using cached click-8.1.8-py3-none-any.whl (98 kB)\n",
            "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
            "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "   ---------------------------------------- 0.0/78.5 kB ? eta -:--:--\n",
            "   ---------------------------------------- 78.5/78.5 kB 4.6 MB/s eta 0:00:00\n",
            "Installing collected packages: tqdm, regex, joblib, click, nltk\n",
            "Successfully installed click-8.1.8 joblib-1.4.2 nltk-3.9.1 regex-2024.11.6 tqdm-4.67.1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\BHANUTEJA\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\BHANUTEJA\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\BHANUTEJA\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\BHANUTEJA\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>cleaned_reviews</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>one reviewer mentioned watching oz episode you...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>wonderful little production br br filming tech...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>thought wonderful way spend time hot summer we...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>basically there family little boy jake think t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>petter matteis love time money visually stunni...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review  \\\n",
              "0  One of the other reviewers has mentioned that ...   \n",
              "1  A wonderful little production. <br /><br />The...   \n",
              "2  I thought this was a wonderful way to spend ti...   \n",
              "3  Basically there's a family where a little boy ...   \n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...   \n",
              "\n",
              "                                     cleaned_reviews  \n",
              "0  one reviewer mentioned watching oz episode you...  \n",
              "1  wonderful little production br br filming tech...  \n",
              "2  thought wonderful way spend time hot summer we...  \n",
              "3  basically there family little boy jake think t...  \n",
              "4  petter matteis love time money visually stunni...  "
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "!pip install nltk\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "# Download the punkt_tab data package\n",
        "nltk.download('punkt_tab') # This line is added to download the required data\n",
        "\n",
        "# Initialize lemmatizer and stopwords\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Function to clean text\n",
        "def clean_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove non-alphabetic characters (punctuation, numbers, etc.)\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "\n",
        "    # Tokenize the text\n",
        "    words = nltk.word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords and lemmatize\n",
        "    cleaned_text = ' '.join([lemmatizer.lemmatize(word) for word in words if word not in stop_words])\n",
        "\n",
        "    return cleaned_text\n",
        "\n",
        "# Apply the cleaning function to the review text column (replace 'review' with your actual column name)\n",
        "df['cleaned_reviews'] = df['review'].apply(clean_text)\n",
        "\n",
        "# Display the first few rows of the cleaned data\n",
        "df[['review', 'cleaned_reviews']].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed80IoB7ewwr"
      },
      "source": [
        "**4. Text Vectorization**\n",
        "\n",
        "To prepare the text for a machine learning model, we need to convert the cleaned text into numerical format.\n",
        "\n",
        "This is done using text vectorization techniques like:\n",
        "\n",
        "**Bag of Words (BoW):** Represents text data as a matrix of word counts.\n",
        "\n",
        "**TF-IDF:** Weighs words based on their frequency and importance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LdSTfnwneb45",
        "outputId": "472a642f-47b4-44f9-9ba3-194ae81d86ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.6.1-cp312-cp312-win_amd64.whl.metadata (15 kB)\n",
            "Requirement already satisfied: numpy>=1.19.5 in c:\\udemy\\python\\venv\\lib\\site-packages (from scikit-learn) (2.2.2)\n",
            "Collecting scipy>=1.6.0 (from scikit-learn)\n",
            "  Downloading scipy-1.15.1-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
            "     ---------------------------------------- 0.0/60.8 kB ? eta -:--:--\n",
            "     ------ --------------------------------- 10.2/60.8 kB ? eta -:--:--\n",
            "     -------------------------------- ----- 51.2/60.8 kB 871.5 kB/s eta 0:00:01\n",
            "     -------------------------------------- 60.8/60.8 kB 814.9 kB/s eta 0:00:00\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\udemy\\python\\venv\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
            "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
            "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
            "Downloading scikit_learn-1.6.1-cp312-cp312-win_amd64.whl (11.1 MB)\n",
            "   ---------------------------------------- 0.0/11.1 MB ? eta -:--:--\n",
            "   - -------------------------------------- 0.3/11.1 MB 5.9 MB/s eta 0:00:02\n",
            "   -- ------------------------------------- 0.7/11.1 MB 8.8 MB/s eta 0:00:02\n",
            "   --- ------------------------------------ 1.1/11.1 MB 8.4 MB/s eta 0:00:02\n",
            "   ----- ---------------------------------- 1.5/11.1 MB 8.5 MB/s eta 0:00:02\n",
            "   ------- -------------------------------- 2.0/11.1 MB 8.9 MB/s eta 0:00:02\n",
            "   -------- ------------------------------- 2.4/11.1 MB 8.9 MB/s eta 0:00:01\n",
            "   --------- ------------------------------ 2.7/11.1 MB 8.3 MB/s eta 0:00:02\n",
            "   ---------- ----------------------------- 3.0/11.1 MB 8.0 MB/s eta 0:00:02\n",
            "   ------------ --------------------------- 3.4/11.1 MB 8.1 MB/s eta 0:00:01\n",
            "   ------------- -------------------------- 3.8/11.1 MB 8.0 MB/s eta 0:00:01\n",
            "   --------------- ------------------------ 4.2/11.1 MB 8.1 MB/s eta 0:00:01\n",
            "   ---------------- ----------------------- 4.6/11.1 MB 8.1 MB/s eta 0:00:01\n",
            "   ----------------- ---------------------- 4.9/11.1 MB 8.0 MB/s eta 0:00:01\n",
            "   ------------------ --------------------- 5.3/11.1 MB 8.2 MB/s eta 0:00:01\n",
            "   -------------------- ------------------- 5.7/11.1 MB 8.2 MB/s eta 0:00:01\n",
            "   --------------------- ------------------ 6.1/11.1 MB 8.3 MB/s eta 0:00:01\n",
            "   ----------------------- ---------------- 6.6/11.1 MB 8.2 MB/s eta 0:00:01\n",
            "   ------------------------- -------------- 7.0/11.1 MB 8.2 MB/s eta 0:00:01\n",
            "   -------------------------- ------------- 7.4/11.1 MB 8.3 MB/s eta 0:00:01\n",
            "   --------------------------- ------------ 7.8/11.1 MB 8.3 MB/s eta 0:00:01\n",
            "   ----------------------------- ---------- 8.1/11.1 MB 8.2 MB/s eta 0:00:01\n",
            "   ------------------------------ --------- 8.5/11.1 MB 8.3 MB/s eta 0:00:01\n",
            "   -------------------------------- ------- 9.0/11.1 MB 8.3 MB/s eta 0:00:01\n",
            "   --------------------------------- ------ 9.3/11.1 MB 8.3 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 9.7/11.1 MB 8.3 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 10.1/11.1 MB 8.3 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 10.6/11.1 MB 8.5 MB/s eta 0:00:01\n",
            "   ---------------------------------------  10.9/11.1 MB 8.4 MB/s eta 0:00:01\n",
            "   ---------------------------------------  11.1/11.1 MB 8.5 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 11.1/11.1 MB 8.2 MB/s eta 0:00:00\n",
            "Downloading scipy-1.15.1-cp312-cp312-win_amd64.whl (43.6 MB)\n",
            "   ---------------------------------------- 0.0/43.6 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.4/43.6 MB 11.6 MB/s eta 0:00:04\n",
            "    --------------------------------------- 0.8/43.6 MB 8.7 MB/s eta 0:00:05\n",
            "   - -------------------------------------- 1.2/43.6 MB 8.5 MB/s eta 0:00:05\n",
            "   - -------------------------------------- 1.6/43.6 MB 8.7 MB/s eta 0:00:05\n",
            "   - -------------------------------------- 2.0/43.6 MB 8.4 MB/s eta 0:00:05\n",
            "   -- ------------------------------------- 2.3/43.6 MB 8.2 MB/s eta 0:00:06\n",
            "   -- ------------------------------------- 2.7/43.6 MB 8.2 MB/s eta 0:00:06\n",
            "   -- ------------------------------------- 3.2/43.6 MB 8.4 MB/s eta 0:00:05\n",
            "   --- ------------------------------------ 3.6/43.6 MB 8.4 MB/s eta 0:00:05\n",
            "   --- ------------------------------------ 4.1/43.6 MB 8.4 MB/s eta 0:00:05\n",
            "   ---- ----------------------------------- 4.4/43.6 MB 8.3 MB/s eta 0:00:05\n",
            "   ---- ----------------------------------- 4.9/43.6 MB 8.4 MB/s eta 0:00:05\n",
            "   ---- ----------------------------------- 5.3/43.6 MB 8.4 MB/s eta 0:00:05\n",
            "   ----- ---------------------------------- 5.7/43.6 MB 8.5 MB/s eta 0:00:05\n",
            "   ----- ---------------------------------- 6.2/43.6 MB 8.7 MB/s eta 0:00:05\n",
            "   ------ --------------------------------- 6.8/43.6 MB 8.8 MB/s eta 0:00:05\n",
            "   ------ --------------------------------- 7.2/43.6 MB 8.9 MB/s eta 0:00:05\n",
            "   ------ --------------------------------- 7.6/43.6 MB 8.9 MB/s eta 0:00:05\n",
            "   ------- -------------------------------- 7.9/43.6 MB 8.5 MB/s eta 0:00:05\n",
            "   ------- -------------------------------- 8.2/43.6 MB 8.4 MB/s eta 0:00:05\n",
            "   ------- -------------------------------- 8.5/43.6 MB 8.4 MB/s eta 0:00:05\n",
            "   -------- ------------------------------- 8.9/43.6 MB 8.4 MB/s eta 0:00:05\n",
            "   -------- ------------------------------- 9.4/43.6 MB 8.3 MB/s eta 0:00:05\n",
            "   --------- ------------------------------ 9.8/43.6 MB 8.5 MB/s eta 0:00:04\n",
            "   --------- ------------------------------ 10.2/43.6 MB 8.5 MB/s eta 0:00:04\n",
            "   --------- ------------------------------ 10.7/43.6 MB 8.5 MB/s eta 0:00:04\n",
            "   ---------- ----------------------------- 11.2/43.6 MB 8.6 MB/s eta 0:00:04\n",
            "   ---------- ----------------------------- 11.6/43.6 MB 8.5 MB/s eta 0:00:04\n",
            "   ---------- ----------------------------- 11.9/43.6 MB 8.5 MB/s eta 0:00:04\n",
            "   ----------- ---------------------------- 12.3/43.6 MB 8.7 MB/s eta 0:00:04\n",
            "   ----------- ---------------------------- 12.7/43.6 MB 8.8 MB/s eta 0:00:04\n",
            "   ------------ --------------------------- 13.1/43.6 MB 9.0 MB/s eta 0:00:04\n",
            "   ------------ --------------------------- 13.4/43.6 MB 8.7 MB/s eta 0:00:04\n",
            "   ------------ --------------------------- 13.9/43.6 MB 8.8 MB/s eta 0:00:04\n",
            "   ------------- -------------------------- 14.4/43.6 MB 8.7 MB/s eta 0:00:04\n",
            "   ------------- -------------------------- 14.8/43.6 MB 9.0 MB/s eta 0:00:04\n",
            "   ------------- -------------------------- 15.1/43.6 MB 9.0 MB/s eta 0:00:04\n",
            "   -------------- ------------------------- 15.6/43.6 MB 9.0 MB/s eta 0:00:04\n",
            "   -------------- ------------------------- 16.1/43.6 MB 9.0 MB/s eta 0:00:04\n",
            "   -------------- ------------------------- 16.3/43.6 MB 8.6 MB/s eta 0:00:04\n",
            "   --------------- ------------------------ 16.6/43.6 MB 8.6 MB/s eta 0:00:04\n",
            "   --------------- ------------------------ 17.0/43.6 MB 8.5 MB/s eta 0:00:04\n",
            "   ---------------- ----------------------- 17.5/43.6 MB 8.6 MB/s eta 0:00:04\n",
            "   ---------------- ----------------------- 17.8/43.6 MB 8.6 MB/s eta 0:00:03\n",
            "   ---------------- ----------------------- 18.3/43.6 MB 9.0 MB/s eta 0:00:03\n",
            "   ----------------- ---------------------- 18.7/43.6 MB 9.4 MB/s eta 0:00:03\n",
            "   ----------------- ---------------------- 19.2/43.6 MB 9.2 MB/s eta 0:00:03\n",
            "   ------------------ --------------------- 19.6/43.6 MB 9.4 MB/s eta 0:00:03\n",
            "   ------------------ --------------------- 20.0/43.6 MB 9.2 MB/s eta 0:00:03\n",
            "   ------------------ --------------------- 20.4/43.6 MB 9.2 MB/s eta 0:00:03\n",
            "   ------------------- -------------------- 20.9/43.6 MB 9.1 MB/s eta 0:00:03\n",
            "   ------------------- -------------------- 21.2/43.6 MB 9.1 MB/s eta 0:00:03\n",
            "   ------------------- -------------------- 21.6/43.6 MB 9.0 MB/s eta 0:00:03\n",
            "   -------------------- ------------------- 21.9/43.6 MB 9.1 MB/s eta 0:00:03\n",
            "   -------------------- ------------------- 22.2/43.6 MB 8.8 MB/s eta 0:00:03\n",
            "   -------------------- ------------------- 22.4/43.6 MB 8.6 MB/s eta 0:00:03\n",
            "   -------------------- ------------------- 22.8/43.6 MB 8.6 MB/s eta 0:00:03\n",
            "   --------------------- ------------------ 23.3/43.6 MB 8.6 MB/s eta 0:00:03\n",
            "   --------------------- ------------------ 23.8/43.6 MB 8.8 MB/s eta 0:00:03\n",
            "   ---------------------- ----------------- 24.2/43.6 MB 8.7 MB/s eta 0:00:03\n",
            "   ---------------------- ----------------- 24.6/43.6 MB 8.7 MB/s eta 0:00:03\n",
            "   ---------------------- ----------------- 25.0/43.6 MB 8.6 MB/s eta 0:00:03\n",
            "   ----------------------- ---------------- 25.4/43.6 MB 8.8 MB/s eta 0:00:03\n",
            "   ----------------------- ---------------- 25.8/43.6 MB 8.7 MB/s eta 0:00:03\n",
            "   ----------------------- ---------------- 26.0/43.6 MB 8.5 MB/s eta 0:00:03\n",
            "   ------------------------ --------------- 26.4/43.6 MB 8.5 MB/s eta 0:00:03\n",
            "   ------------------------ --------------- 26.8/43.6 MB 8.7 MB/s eta 0:00:02\n",
            "   ------------------------ --------------- 27.2/43.6 MB 8.7 MB/s eta 0:00:02\n",
            "   ------------------------- -------------- 27.6/43.6 MB 8.5 MB/s eta 0:00:02\n",
            "   ------------------------- -------------- 28.0/43.6 MB 8.5 MB/s eta 0:00:02\n",
            "   -------------------------- ------------- 28.4/43.6 MB 8.5 MB/s eta 0:00:02\n",
            "   -------------------------- ------------- 28.8/43.6 MB 8.4 MB/s eta 0:00:02\n",
            "   -------------------------- ------------- 29.3/43.6 MB 8.3 MB/s eta 0:00:02\n",
            "   --------------------------- ------------ 29.7/43.6 MB 8.5 MB/s eta 0:00:02\n",
            "   --------------------------- ------------ 30.1/43.6 MB 8.3 MB/s eta 0:00:02\n",
            "   --------------------------- ------------ 30.4/43.6 MB 8.4 MB/s eta 0:00:02\n",
            "   ---------------------------- ----------- 30.8/43.6 MB 8.4 MB/s eta 0:00:02\n",
            "   ---------------------------- ----------- 31.2/43.6 MB 8.5 MB/s eta 0:00:02\n",
            "   ---------------------------- ----------- 31.6/43.6 MB 8.5 MB/s eta 0:00:02\n",
            "   ----------------------------- ---------- 31.9/43.6 MB 8.4 MB/s eta 0:00:02\n",
            "   ----------------------------- ---------- 32.3/43.6 MB 8.3 MB/s eta 0:00:02\n",
            "   ----------------------------- ---------- 32.7/43.6 MB 8.5 MB/s eta 0:00:02\n",
            "   ------------------------------ --------- 33.1/43.6 MB 8.5 MB/s eta 0:00:02\n",
            "   ------------------------------ --------- 33.5/43.6 MB 8.5 MB/s eta 0:00:02\n",
            "   ------------------------------- -------- 33.9/43.6 MB 8.4 MB/s eta 0:00:02\n",
            "   ------------------------------- -------- 34.3/43.6 MB 8.5 MB/s eta 0:00:02\n",
            "   ------------------------------- -------- 34.6/43.6 MB 8.4 MB/s eta 0:00:02\n",
            "   -------------------------------- ------- 35.0/43.6 MB 8.4 MB/s eta 0:00:02\n",
            "   -------------------------------- ------- 35.3/43.6 MB 8.3 MB/s eta 0:00:02\n",
            "   -------------------------------- ------- 35.5/43.6 MB 8.2 MB/s eta 0:00:01\n",
            "   -------------------------------- ------- 35.8/43.6 MB 8.1 MB/s eta 0:00:01\n",
            "   --------------------------------- ------ 36.1/43.6 MB 8.1 MB/s eta 0:00:01\n",
            "   --------------------------------- ------ 36.4/43.6 MB 8.1 MB/s eta 0:00:01\n",
            "   --------------------------------- ------ 36.8/43.6 MB 8.1 MB/s eta 0:00:01\n",
            "   ---------------------------------- ----- 37.1/43.6 MB 8.2 MB/s eta 0:00:01\n",
            "   ---------------------------------- ----- 37.4/43.6 MB 8.0 MB/s eta 0:00:01\n",
            "   ---------------------------------- ----- 37.8/43.6 MB 8.1 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 38.2/43.6 MB 8.1 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 38.5/43.6 MB 8.0 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 38.8/43.6 MB 8.0 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 39.2/43.6 MB 8.0 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 39.5/43.6 MB 7.9 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 39.8/43.6 MB 7.8 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 40.0/43.6 MB 7.6 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 40.3/43.6 MB 7.6 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 40.7/43.6 MB 7.5 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 41.1/43.6 MB 7.5 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 41.4/43.6 MB 7.4 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 41.7/43.6 MB 7.4 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 42.0/43.6 MB 7.4 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 42.4/43.6 MB 7.4 MB/s eta 0:00:01\n",
            "   ---------------------------------------  42.7/43.6 MB 7.4 MB/s eta 0:00:01\n",
            "   ---------------------------------------  43.0/43.6 MB 7.4 MB/s eta 0:00:01\n",
            "   ---------------------------------------  43.3/43.6 MB 7.3 MB/s eta 0:00:01\n",
            "   ---------------------------------------  43.6/43.6 MB 7.3 MB/s eta 0:00:01\n",
            "   ---------------------------------------  43.6/43.6 MB 7.3 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 43.6/43.6 MB 6.8 MB/s eta 0:00:00\n",
            "Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
            "Installing collected packages: threadpoolctl, scipy, scikit-learn\n",
            "Successfully installed scikit-learn-1.6.1 scipy-1.15.1 threadpoolctl-3.5.0\n",
            "(50000, 5000)\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-learn\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Initialize the TF-IDF Vectorizer\n",
        "vectorizer = TfidfVectorizer(max_features=5000)  # Use the top 5000 features\n",
        "\n",
        "# Fit and transform the cleaned reviews into a numerical format\n",
        "X = vectorizer.fit_transform(df['cleaned_reviews']).toarray()\n",
        "\n",
        "# Display the shape of the resulting matrix\n",
        "print(X.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eG2FQTLVfmF1"
      },
      "source": [
        "**5. Model Training**\n",
        "\n",
        "We'll train a machine learning model to predict the sentiment of the movie reviews (positive or negative). For sentiment analysis, Logistic Regression is a good starting point, but we can also try other models like Naive Bayes or Support Vector Machines (SVM).\n",
        "\n",
        "We’ll follow these steps:\n",
        "\n",
        "**Split** the dataset into training and testing sets.\n",
        "\n",
        "**Train** the model using the training data.\n",
        "\n",
        "**Evaluate** the model using the test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AxIZ2xhEgPuQ",
        "outputId": "831f75e6-246f-4c00-bbcc-3840fcda77ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.8847\n",
            "Confusion Matrix:\n",
            "[[4321  640]\n",
            " [ 513 4526]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.89      0.87      0.88      4961\n",
            "    positive       0.88      0.90      0.89      5039\n",
            "\n",
            "    accuracy                           0.88     10000\n",
            "   macro avg       0.88      0.88      0.88     10000\n",
            "weighted avg       0.88      0.88      0.88     10000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# Assuming 'sentiment' is the column that contains the labels (0 for negative, 1 for positive)\n",
        "# Replace 'sentiment' with the actual column name in your dataset\n",
        "y = df['sentiment']  # Labels\n",
        "X = vectorizer.fit_transform(df['cleaned_reviews']).toarray()  # Features\n",
        "\n",
        "# Split the data into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "class_report = classification_report(y_test, y_pred)\n",
        "\n",
        "# Display the results\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "print(\"Classification Report:\")\n",
        "print(class_report)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvQMtqxLg8L4"
      },
      "source": [
        "**6. Testing the Model on New Data**\n",
        "\n",
        "To test the model on new, unseen data (i.e., a new movie review), we'll follow these steps:\n",
        "\n",
        "Clean the new review text (using the same cleaning function we used before).\n",
        "\n",
        "Vectorize the cleaned text using the same TF-IDF vectorizer.\n",
        "\n",
        "Make predictions using the trained model.\n",
        "\n",
        "Here’s the code to test the model on new reviews:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aur2TmYchAVO",
        "outputId": "4913bac9-b63d-45ab-d342-7054515b7a1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted sentiment: Negative\n"
          ]
        }
      ],
      "source": [
        "# Function to predict sentiment for a new review\n",
        "def predict_sentiment(new_review):\n",
        "    # Clean the new review\n",
        "    cleaned_review = clean_text(new_review)\n",
        "\n",
        "    # Vectorize the cleaned review\n",
        "    vectorized_review = vectorizer.transform([cleaned_review]).toarray()\n",
        "\n",
        "    # Make prediction\n",
        "    prediction = model.predict(vectorized_review)\n",
        "\n",
        "    # Return the sentiment (0 for negative, 1 for positive)\n",
        "    return \"Positive\" if prediction == 1 else \"Negative\"\n",
        "\n",
        "# Test the model on a new review\n",
        "new_review = \"The movie was a complete disaster. The plot was predictable and the acting was subpar.\"\n",
        "print(f\"Predicted sentiment: {predict_sentiment(new_review)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufSiRqL7kXPD",
        "outputId": "6ce4ae3f-a397-4193-b49f-c1f8e2f1116f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original: ['positive' 'positive' 'negative' ... 'positive' 'negative' 'positive']\n",
            "Predicted: ['negative' 'positive' 'negative' ... 'positive' 'negative' 'positive']\n",
            "Correctly predicted percentage: 88.47%\n"
          ]
        }
      ],
      "source": [
        "# Make predictions on the test set\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Print the original and predicted values (for debugging purposes)\n",
        "print(\"Original:\", y_test.values)\n",
        "print(\"Predicted:\", predictions)\n",
        "\n",
        "# Calculate the number of correct predictions\n",
        "correct_prediction = 0\n",
        "for i in range(len(predictions)):\n",
        "    if predictions[i] == y_test.iloc[i]:  # Comparing the predictions to the true labels\n",
        "        correct_prediction += 1\n",
        "\n",
        "# Calculate and print the percentage of correct predictions\n",
        "correct_percentage = (correct_prediction * 100) / len(predictions)\n",
        "print(f\"Correctly predicted percentage: {correct_percentage:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuZKAoFgr-3Z"
      },
      "source": [
        "# **Importing and Training the Naive Bayes Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "s64qHiugq8Tw"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "9uOvVp0srT-e"
      },
      "outputs": [],
      "source": [
        "# Vectorize the reviews using TF-IDF\n",
        "vectorizer = TfidfVectorizer(max_features=5000)  # You can adjust max_features based on the dataset size\n",
        "X = vectorizer.fit_transform(df['cleaned_reviews']).toarray()  # Use 'cleaned_reviews' column\n",
        "\n",
        "# The target variable is the sentiment (positive or negative)\n",
        "Y = df['sentiment']  # Assuming sentiment column is already encoded as 0 and 1\n",
        "\n",
        "# Split the data into training and test sets (80% training, 20% test)\n",
        "X_train_tfidf, X_test_tfidf, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpJcMxryro25",
        "outputId": "1aab8568-c01d-4439-e261-210d142dc296"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 85.20%\n",
            "Confusion Matrix:\n",
            "[[4207  754]\n",
            " [ 726 4313]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.85      0.85      0.85      4961\n",
            "    positive       0.85      0.86      0.85      5039\n",
            "\n",
            "    accuracy                           0.85     10000\n",
            "   macro avg       0.85      0.85      0.85     10000\n",
            "weighted avg       0.85      0.85      0.85     10000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# Initialize the Naive Bayes model\n",
        "nb_model = MultinomialNB()\n",
        "\n",
        "# Train the model\n",
        "nb_model.fit(X_train_tfidf, Y_train)\n",
        "\n",
        "# Make predictions\n",
        "nb_predictions = nb_model.predict(X_test_tfidf)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(Y_test, nb_predictions)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Confusion Matrix\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(Y_test, nb_predictions))\n",
        "\n",
        "# Classification Report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(Y_test, nb_predictions))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GEKE2-n4tHBD",
        "outputId": "bd36e39b-4dfb-436b-b4c3-5143fb789cf3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Review 1: This movie was absolutely amazing, I loved every second of it! The acting was great and the plot was engaging.\n",
            "Predicted Sentiment: Negative\n",
            "\n",
            "Review 2: The movie was terrible. The plot was boring, and the acting was subpar. I wouldn't recommend it.\n",
            "Predicted Sentiment: Negative\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Import the necessary libraries (if not already done)\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Assuming you have the 'df' DataFrame and 'clean_text' function already defined as before\n",
        "# Define and fit the TfidfVectorizer on the training data\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)  # You can adjust max_features as needed\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(df['cleaned_reviews'])  # Use the cleaned reviews from your DataFrame\n",
        "\n",
        "# Example new reviews\n",
        "new_reviews = [\n",
        "    \"This movie was absolutely amazing, I loved every second of it! The acting was great and the plot was engaging.\",\n",
        "    \"The movie was terrible. The plot was boring, and the acting was subpar. I wouldn't recommend it.\"\n",
        "]\n",
        "\n",
        "# Preprocess the new reviews using the same cleaning function\n",
        "new_reviews_cleaned = [clean_text(review) for review in new_reviews]\n",
        "\n",
        "# Convert the cleaned reviews into the same feature format as the training data (TF-IDF)\n",
        "new_reviews_tfidf = tfidf_vectorizer.transform(new_reviews_cleaned)\n",
        "\n",
        "# Use the trained Naive Bayes model to predict the sentiment of the new reviews\n",
        "predictions = nb_model.predict(new_reviews_tfidf)\n",
        "\n",
        "# Display the predictions\n",
        "for i, review in enumerate(new_reviews):\n",
        "    print(f\"Review {i+1}: {review}\")\n",
        "    print(f\"Predicted Sentiment: {'Positive' if predictions[i] == 1 else 'Negative'}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Z6P9FOxz3G9L"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset (make sure the correct path to the file is provided)\n",
        "df = pd.read_csv('C:/Users/BHANUTEJA/Desktop/Sentiment Analysis/IMDB Dataset.csv')\n",
        "\n",
        "# If your dataset is already in the environment, just load it into df\n",
        "# For example, if it’s in a file named 'reviews.csv' in the same folder:\n",
        "df = pd.read_csv('IMDB Dataset.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clVc-VRW4QY4",
        "outputId": "ae166698-5ea7-492b-eb3e-53c74d4e78ed"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['review', 'sentiment'], dtype='object')"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Check the first few rows of the dataset and its columns\n",
        "df.head()\n",
        "df.columns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jSccIUg4UI8",
        "outputId": "30648084-4fbd-490f-afcf-f77155d07832"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['review', 'sentiment'], dtype='object')\n"
          ]
        }
      ],
      "source": [
        "# Check the columns of the dataframe\n",
        "print(df.columns)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lg0t3H16V3j",
        "outputId": "6ad137a2-b884-4636-f136-46a441c9ffca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                              review  \\\n",
            "0  One of the other reviewers has mentioned that ...   \n",
            "1  A wonderful little production. <br /><br />The...   \n",
            "2  I thought this was a wonderful way to spend ti...   \n",
            "3  Basically there's a family where a little boy ...   \n",
            "4  Petter Mattei's \"Love in the Time of Money\" is...   \n",
            "\n",
            "                                     cleaned_reviews  \n",
            "0  one reviewer mentioned watching oz episode you...  \n",
            "1  wonderful little production br br filming tech...  \n",
            "2  thought wonderful way spend time hot summer we...  \n",
            "3  basically there family little boy jake think t...  \n",
            "4  petter matteis love time money visually stunni...  \n"
          ]
        }
      ],
      "source": [
        "# Assuming the clean_text function is already defined\n",
        "df['cleaned_reviews'] = df['review'].apply(clean_text)\n",
        "\n",
        "# Check the first few rows to make sure it's working\n",
        "print(df[['review', 'cleaned_reviews']].head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "mz3e87W_6wt2"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Initialize the vectorizer with max features and n-grams (unigrams and bigrams)\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
        "\n",
        "# Fit and transform the training data\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(df['cleaned_reviews'])\n",
        "\n",
        "# If you have a separate test dataset, transform it with the same vectorizer\n",
        "# Assuming you have a column 'cleaned_reviews_test' for testing data, if not you can split the data manually\n",
        "X_test_tfidf = tfidf_vectorizer.transform(df['cleaned_reviews'])  # For the same dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-yNKTHw17ujf",
        "outputId": "8b39fd56-2d1c-4093-bf98-ada260cbdde7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 85.48%\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split your data into training and testing sets\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(df['cleaned_reviews'], df['sentiment'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Now, transform both train and test data\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 3))\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Train the Naive Bayes model\n",
        "nb_model.fit(X_train_tfidf, Y_train)\n",
        "\n",
        "# Predict on the test data\n",
        "Y_pred = nb_model.predict(X_test_tfidf)\n",
        "\n",
        "# Check accuracy\n",
        "accuracy = (Y_pred == Y_test).mean()\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evVMbUrp62Au",
        "outputId": "20daca5d-2713-42f4-efa5-61278c8e8e39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(40000, 5000)\n",
            "(10000, 5000)\n"
          ]
        }
      ],
      "source": [
        "# Check the shape of the transformed data\n",
        "print(X_train_tfidf.shape)\n",
        "print(X_test_tfidf.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTv8Eo6y7PI6"
      },
      "source": [
        "**Train the Naive Bayes model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSf5xaof7M1z",
        "outputId": "26b32201-9a5f-4bf8-8646-8f6e73e27f03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 86.35%\n",
            "Confusion Matrix:\n",
            "[[4405  556]\n",
            " [ 809 4230]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.84      0.89      0.87      4961\n",
            "    positive       0.88      0.84      0.86      5039\n",
            "\n",
            "    accuracy                           0.86     10000\n",
            "   macro avg       0.86      0.86      0.86     10000\n",
            "weighted avg       0.86      0.86      0.86     10000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# Assuming you start with raw text and sentiment labels\n",
        "X_raw = df['review']  # Replace 'review' with the column name containing text data\n",
        "Y = df['sentiment']\n",
        "\n",
        "# Split the data\n",
        "X_train_raw, X_test_raw, Y_train, Y_test = train_test_split(X_raw, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Apply TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_raw)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test_raw)\n",
        "\n",
        "# Initialize and train the Naive Bayes model\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train_tfidf, Y_train)\n",
        "\n",
        "# Make predictions and evaluate\n",
        "predictions = nb_model.predict(X_test_tfidf)\n",
        "print(f\"Accuracy: {accuracy_score(Y_test, predictions) * 100:.2f}%\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(Y_test, predictions))\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(Y_test, predictions))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPJJEqlq7bDo",
        "outputId": "7d1078cf-b5a3-4a5f-c1ce-1cfcb429194c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Review: This movie was amazing! I loved every bit of it, definitely worth watching.\n",
            "Sentiment: Negative\n",
            "\n",
            "Review: Worst movie ever, a total waste of time.\n",
            "Sentiment: Negative\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Define new reviews\n",
        "new_reviews = [\n",
        "    \"This movie was amazing! I loved every bit of it, definitely worth watching.\",\n",
        "    \"Worst movie ever, a total waste of time.\"\n",
        "]\n",
        "\n",
        "# Clean the new reviews\n",
        "new_reviews_cleaned = [clean_text(review) for review in new_reviews]\n",
        "\n",
        "# Transform the cleaned reviews using the same TF-IDF vectorizer\n",
        "new_reviews_tfidf = tfidf_vectorizer.transform(new_reviews_cleaned)\n",
        "\n",
        "# Make predictions on the new reviews\n",
        "new_predictions = nb_model.predict(new_reviews_tfidf)\n",
        "\n",
        "# Output the predictions\n",
        "for review, prediction in zip(new_reviews, new_predictions):\n",
        "    sentiment = \"Positive\" if prediction == 1 else \"Negative\"\n",
        "    print(f\"Review: {review}\\nSentiment: {sentiment}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "re4jIQLRBoZZ"
      },
      "source": [
        "**Hypertuning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gyuNcPovAk3R",
        "outputId": "c54c9aef-bcff-4a3b-84f6-1ad85431f1c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
            "Best Hyperparameters: {'alpha': 0.1}\n",
            "Accuracy after Hyperparameter Tuning: 85.59%\n",
            "Confusion Matrix:\n",
            "[[4182  779]\n",
            " [ 662 4377]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.86      0.84      0.85      4961\n",
            "    positive       0.85      0.87      0.86      5039\n",
            "\n",
            "    accuracy                           0.86     10000\n",
            "   macro avg       0.86      0.86      0.86     10000\n",
            "weighted avg       0.86      0.86      0.86     10000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Prepare the TF-IDF vectorizer again if it's not already done\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Define the Naive Bayes model\n",
        "nb_model = MultinomialNB()\n",
        "\n",
        "# Define the parameter grid for hyperparameter tuning\n",
        "param_grid = {\n",
        "    'alpha': [0.1, 0.5, 1.0, 2.0, 5.0]  # Alpha is the smoothing parameter for Naive Bayes\n",
        "}\n",
        "\n",
        "# Set up the GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=nb_model, param_grid=param_grid, cv=5, verbose=1, n_jobs=-1)\n",
        "\n",
        "# Fit GridSearchCV to the training data\n",
        "grid_search.fit(X_train_tfidf, Y_train)\n",
        "\n",
        "# Get the best hyperparameters\n",
        "print(f\"Best Hyperparameters: {grid_search.best_params_}\")\n",
        "\n",
        "# Use the best model from GridSearchCV\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Predict on the test data\n",
        "Y_pred = best_model.predict(X_test_tfidf)\n",
        "\n",
        "# Check accuracy\n",
        "accuracy = (Y_pred == Y_test).mean()\n",
        "print(f\"Accuracy after Hyperparameter Tuning: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# If needed, you can check classification report and confusion matrix as well:\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(Y_test, Y_pred))\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(Y_test, Y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1y5eBGgB7LX",
        "outputId": "c7936df4-ac7c-499d-bb72-9a15416e23a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Review: The movie was fantastic! The plot was gripping and the acting was superb. Highly recommend it!\n",
            "Sentiment: Negative\n",
            "\n",
            "Review: I absolutely loved this film. The direction and screenplay were top-notch, a must-watch for movie lovers.\n",
            "Sentiment: Negative\n",
            "\n",
            "Review: A waste of two hours. The movie was filled with clichés and had no real depth or emotion.\n",
            "Sentiment: Negative\n",
            "\n",
            "Review: The worst movie I've seen in a long time. The acting was subpar and the plot was all over the place.\n",
            "Sentiment: Negative\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Assuming you have already cleaned the reviews and have training data (X_train_tfidf and Y_train)\n",
        "\n",
        "# Train the Naive Bayes model on the training data\n",
        "nb_model.fit(X_train_tfidf, Y_train)\n",
        "\n",
        "# New reviews (5 positive and 5 negative)\n",
        "new_reviews = [\n",
        "    \"The movie was fantastic! The plot was gripping and the acting was superb. Highly recommend it!\",  # Positive\n",
        "    \"I absolutely loved this film. The direction and screenplay were top-notch, a must-watch for movie lovers.\",  # Positive\n",
        "   \"A waste of two hours. The movie was filled with clichés and had no real depth or emotion.\",  # Negative\n",
        "    \"The worst movie I've seen in a long time. The acting was subpar and the plot was all over the place.\"  # Negative\n",
        "]\n",
        "\n",
        "# Clean the new reviews (using the same cleaning function from before)\n",
        "new_reviews_cleaned = [clean_text(review) for review in new_reviews]\n",
        "\n",
        "# Convert the cleaned reviews into the same feature format as the training data (TF-IDF)\n",
        "new_reviews_tfidf = tfidf_vectorizer.transform(new_reviews_cleaned)\n",
        "\n",
        "# Use the trained Naive Bayes model to predict the sentiment of the new reviews\n",
        "new_predictions = nb_model.predict(new_reviews_tfidf)\n",
        "\n",
        "# Display the results\n",
        "for review, prediction in zip(new_reviews, new_predictions):\n",
        "    sentiment = \"Positive\" if prediction == 1 else \"Negative\"\n",
        "    print(f\"Review: {review}\\nSentiment: {sentiment}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cL3fDbdsCp-Q",
        "outputId": "c3a8eacc-845b-4398-ae93-de02bead745c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sentiment\n",
            "positive    25000\n",
            "negative    25000\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(df['sentiment'].value_counts())  # To check the number of positive and negative labels in the dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ExY3xPOC6hE",
        "outputId": "3ce078aa-a829-4285-a848-445b69ad4049"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training data size: (40000,)\n",
            "Test data size: (10000,)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(df['review'], df['sentiment'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Check the shapes of the splits\n",
        "print(\"Training data size:\", X_train.shape)\n",
        "print(\"Test data size:\", X_test.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "SA-g3aibDtyF"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# TF-IDF Vectorization with unigrams, bigrams, and trigrams, and top 8000 features\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=8000, ngram_range=(1, 3))  # Unigrams, bigrams, trigrams\n",
        "\n",
        "# Fit the vectorizer on the training data and transform it\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "\n",
        "# Transform the test data using the same vectorizer\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2f0Mm5EEQCj",
        "outputId": "3fcb336d-6a0f-47fd-a12b-1a481e7cb911"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.8658\n",
            "Confusion Matrix:\n",
            " [[4226  735]\n",
            " [ 607 4432]]\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.87      0.85      0.86      4961\n",
            "    positive       0.86      0.88      0.87      5039\n",
            "\n",
            "    accuracy                           0.87     10000\n",
            "   macro avg       0.87      0.87      0.87     10000\n",
            "weighted avg       0.87      0.87      0.87     10000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Train the Naive Bayes model\n",
        "nb_model = MultinomialNB(alpha=0.1)  # Use the optimal alpha from hyperparameter tuning\n",
        "nb_model.fit(X_train_tfidf, Y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "Y_pred = nb_model.predict(X_test_tfidf)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Accuracy:\", accuracy_score(Y_test, Y_pred))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(Y_test, Y_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(Y_test, Y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mntXNnAsETxK",
        "outputId": "54cab14a-7c09-4300-c63e-76c34b580a43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Review: This movie was fantastic, I loved it!\n",
            "Sentiment: negative\n",
            "\n",
            "Review: Horrible movie, I regret watching it.\n",
            "Sentiment: negative\n",
            "\n",
            "Review: Amazing performance by the actors, highly recommended.\n",
            "Sentiment: negative\n",
            "\n",
            "Review: Complete waste of time, boring and long.\n",
            "Sentiment: negative\n",
            "\n",
            "Review: One of the best movies I've seen, truly captivating!\n",
            "Sentiment: negative\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# New reviews for testing\n",
        "new_reviews = [\n",
        "    \"This movie was fantastic, I loved it!\",\n",
        "    \"Horrible movie, I regret watching it.\",\n",
        "    \"Amazing performance by the actors, highly recommended.\",\n",
        "    \"Complete waste of time, boring and long.\",\n",
        "    \"One of the best movies I've seen, truly captivating!\"\n",
        "]\n",
        "\n",
        "# Clean the new reviews (use the same cleaning function you applied before)\n",
        "new_reviews_cleaned = [clean_text(review) for review in new_reviews]\n",
        "\n",
        "# Transform the new reviews using the same TF-IDF vectorizer\n",
        "new_reviews_tfidf = tfidf_vectorizer.transform(new_reviews_cleaned)\n",
        "\n",
        "# Predict the sentiment of the new reviews\n",
        "new_predictions = nb_model.predict(new_reviews_tfidf)\n",
        "\n",
        "# Print the predictions\n",
        "for review, prediction in zip(new_reviews, new_predictions):\n",
        "    sentiment = \"positive\" if prediction == 1 else \"negative\"\n",
        "    print(f\"Review: {review}\\nSentiment: {sentiment}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMVdFtMmGmrq"
      },
      "source": [
        "# **Using Predefined Models**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: textblob in c:\\udemy\\python\\venv\\lib\\site-packages (0.19.0)\n",
            "Requirement already satisfied: nltk>=3.9 in c:\\udemy\\python\\venv\\lib\\site-packages (from textblob) (3.9.1)\n",
            "Requirement already satisfied: click in c:\\udemy\\python\\venv\\lib\\site-packages (from nltk>=3.9->textblob) (8.1.8)\n",
            "Requirement already satisfied: joblib in c:\\udemy\\python\\venv\\lib\\site-packages (from nltk>=3.9->textblob) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\udemy\\python\\venv\\lib\\site-packages (from nltk>=3.9->textblob) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in c:\\udemy\\python\\venv\\lib\\site-packages (from nltk>=3.9->textblob) (4.67.1)\n",
            "Requirement already satisfied: colorama in c:\\udemy\\python\\venv\\lib\\site-packages (from click->nltk>=3.9->textblob) (0.4.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install textblob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6G-9_k6UGG_-",
        "outputId": "564f1681-f055-40ae-f68e-915fc98eb21b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Review: This movie was amazing! I loved every bit of it. --> Positive Sentiment\n",
            "Review: Worst movie ever, a total waste of time. --> Negative Sentiment\n",
            "Review: It was okay, not great, but not bad either. --> Positive Sentiment\n"
          ]
        }
      ],
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "# Sample reviews\n",
        "reviews = [\n",
        "    \"This movie was amazing! I loved every bit of it.\",\n",
        "    \"Worst movie ever, a total waste of time.\",\n",
        "    \"It was okay, not great, but not bad either.\"\n",
        "]\n",
        "\n",
        "# Loop through each review and determine the sentiment\n",
        "for review in reviews:\n",
        "    blob = TextBlob(review)\n",
        "    sentiment = blob.sentiment.polarity\n",
        "    if sentiment > 0:\n",
        "        print(f\"Review: {review} --> Positive Sentiment\")\n",
        "    elif sentiment < 0:\n",
        "        print(f\"Review: {review} --> Negative Sentiment\")\n",
        "    else:\n",
        "        print(f\"Review: {review} --> Neutral Sentiment\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
